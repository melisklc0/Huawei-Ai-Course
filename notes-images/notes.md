# Huawei Ai Kurs Notlarƒ±

# Ai Overview

Symbolism:   
Connectionism: Simple processing yerine n√∂ronlar kullanƒ±lmalƒ±.  
Behaviorism: √áevreyle etkile≈üim yaptƒ±k√ßa ai kendini geli≈ütirebilir.

**Strong AI:** Bu g√∂r√º≈ü ger√ßek problemi √ß√∂zebilen makineler olabilir der, self aware ve zeki modeller.  
**Weak AI**: Zeki g√∂r√ºn√ºrler ama aslƒ±nda ger√ßek problemleri √ß√∂zemezler. Self awareness yoktur.

**4 element:** data, algorithm, computing power, scenario.  
Bunlar i√ßin de ai ile cloud computing, big data, IoT gibi konularƒ± birle≈ütirmeliyiz.

Genel olarak AI teknolojileri:
- Computer vision
- Speech processing
- Natural language processing (NLP)

Bazƒ± kullanƒ±m alanlarƒ±: medikal, retail(market), autonomous driving vs.

**Full Stack:**  
ModelArts workflow, Mindspore (ai computing network), CANN (chip operator library), Da vinci core mimarisi (+ ARM CPU), 

**Endi≈üeler:**  
Privacy issues: Verilerin nerden geldiƒüi ile ilgili gizlilik endi≈üeleri  
Fake images: G√∂r√ºnt√ºlerin ger√ßek mi fake mi olduƒüunu anlayamamak  
ƒ∞≈üsizlik: Bazƒ± i≈üler ai tarafƒ±ndan devralƒ±nabilir  

**Yazƒ±lƒ±m trendleri:**   
- Frameworks: MindSpore, Tensorflow, Pytorch  
Akademik √ßalƒ±≈ümalarda pytorch daha √ßok √∂ne √ßƒ±karken, end√ºstride tensorflow ta≈üƒ±ma kolaylƒ±ƒüƒ± ile daha √ßok √∂n plana √ßƒ±kmƒ±≈ü.  
- Computer vision: GAN algoritmasƒ± g√∂r√ºnt√º √ºretme   
- NLP field: transformer mimarisi  
-Reinforcement learning field: Alphastar etc.

-> By "all AI scenarios", Huawei means different deployment scenarios for AI, including public clouds, private clouds, edge computing in all forms, industrial IoT devices, and consumer devices.


# Machine Learning Overview

Coverage rate: kurallar ger√ßek d√ºnya verilerine her zaman uyumlu olmayabilir, how much data do the rules can fit?

**Rule-based method:** elimizde deƒüi≈üen bi veriseti varsa kurallar da buna g√∂re s√ºrekli deƒüi≈ümelidir, bu da √ßok karma≈üƒ±k olabilir.  
**Machine learning:** model, verisetini √∂ƒürenerek karar verme mekanizmasƒ±nƒ±, yani kurallarƒ± kendi kendine √∂ƒürenir.

Kompleks ya da a√ßƒ±klanamayan kurallar, deƒüi≈üen senaryolar, deƒüi≈üen veriler vs. gibi durumlarda makine √∂ƒürenmesi tercih edilir.

Fonksiyonun bir ideal mapping function u olmalƒ±dƒ±r. source domain X ile target domain Y yi e≈üler.
Objective function (f): Bu fonksiyon √∂ƒürenilir, direkt olarak elde edilemez. 

Ba≈üka bir (g) fonksiyonu tanƒ±mlayarak f i olabildiƒüince tahmin etmeye √ßalƒ±≈üabiliriz. Bu fonksiyon f'e yakla≈üan bir tahmini fonksiyondur.

## **Temel Kavramlar**
**AI (Artificial Intelligence), Machine Learning (ML), ve Deep Learning (DL) arasƒ±ndaki fark nedir?**

1. **Artificial Intelligence (AI):**
   - **Tanƒ±m:** Bilgisayarlarƒ±n insan gibi d√º≈ü√ºnme, √∂ƒürenme ve karar verme becerilerine sahip olmasƒ±nƒ± saƒülayan geni≈ü bir teknoloji alanƒ±dƒ±r.
   - **Kapsam:** ML ve DL de dahil olmak √ºzere t√ºm zeka tabanlƒ± sistemleri i√ßerir.
   - **√ñrnekler:** Oto-pilot araba, sanal asistanlar (Siri, Alexa), oyun veya strateji oyuncularƒ±.

2. **Machine Learning (ML):**
   - **Tanƒ±m:** AI'nin bir alt k√ºmesidir. Bilgisayarlarƒ±n veriden √∂ƒürenme ve model olu≈üturma s√ºre√ßlerini otomatikle≈ütirir.
   - **Y√∂ntemler:**
     - **G√∂zlemli √ñƒürenme (Supervised Learning):** Etiketlenmi≈ü veriler ile eƒüitilir (√∂r. spam tespiti).
     - **G√∂zlemsiz √ñƒürenme (Unsupervised Learning):** Veri i√ßinde √∂r√ºnt√ºleri tespit eder (√∂r. m√º≈üteri segmentasyonu).
     - **Takviyeli √ñƒürenme (Reinforcement Learning):** √ñd√ºller ve cezalar ile √∂ƒürenir (√∂r. AlphaGo).
   - **√ñrnekler:** Spam filtreleri, tavsiye sistemleri, kredi skoru tahminleri.

3. **Deep Learning (DL):**
   - **Tanƒ±m:** ML'nin bir alt k√ºmesidir. Yapay sinir aƒülarƒ± (YSA) kullanarak veriden √ß√∂z√ºmleri otomatik olarak √∂ƒürenir.
   - **√ñzellikler:**
     - **Derin Aƒülar:** √áoklu katmanlƒ± aƒülar (√∂r. CNN, RNN) kullanƒ±r.
     - **Otomatik √ñznitelik √áƒ±karƒ±mƒ±:** Veri i√ßinden √∂zelliƒüi kendiliƒüinden √∂ƒürenir (√∂r. resimdeki bir kedinin y√ºz √∂zelliklerini).
   - **√ñrnekler:** G√∂r√ºnt√º tanƒ±ma (DeepFace), konu≈üma tanƒ±ma (Siri), √∂neri sistemleri.

### **√ñzet:**
- **AI:** T√ºm zeka tabanlƒ± sistemleri kapsar.
- **ML:** AI'nin bir alt k√ºmesidir, veriden √∂ƒürenmeyi otomatikle≈ütirir.
- **DL:** ML'nin bir alt k√ºmesidir, derin sinir aƒülarƒ± kullanƒ±r ve veriden √∂zelliƒüi kendiliƒüinden √∂ƒürenir.

**Not:** Deep learning, √∂zellikle b√ºy√ºk veri miktarlarƒ±nda ve karma≈üƒ±k g√∂revlerde (√∂r. g√∂r√ºnt√º ve dil i≈üleme) avantaj sahiptir, ancak geleneksel ML y√∂ntemleri k√º√ß√ºk veri setlerinde daha verimli olabilir.


## **Three Major Schools of Thought in AI** (Huawei Confidential belgesine g√∂re):

1. **Symbolism**
   - **Temel D√º≈ü√ºnce:** Zekanƒ±n sembol i≈üleme (matematiksel kurallar, mantƒ±k) ve bilgi temelli bir s√ºrecin √ºr√ºn√º olduƒüu √∂ne s√ºr√ºl√ºr.
   - **Temsilcileri:** Sembolik sistemler, mantƒ±ksal √ßƒ±kartma, traditional rule-based AI.
   - **√ñzellik:** Bilgi, kavramlar ve kurallar sembollerle temsil edilir.

2. **Connectionism**
   - **Temel D√º≈ü√ºnce:** Zeka, beyin gibi sinir aƒülarƒ± (neuronlar) ve baƒülarla √ßalƒ±≈üƒ±r. Sembol i≈ülemine kar≈üƒ± √ßƒ±kƒ±lƒ±r.
   - **Temsilcileri:** Yapay sinir aƒülarƒ± (ANN), deep learning.
   - **√ñzellik:** Otomatik √∂zelliƒüe √ßƒ±karma ve b√ºy√ºk veri miktarƒ± gereklilikleri.

3. **Behaviorism**
   - **Temel D√º≈ü√ºnce:** Zeka, algƒ± ve eylem arasƒ±ndaki doƒürudan etkile≈üim ile olu≈üur. Bilgi temsili veya √ßƒ±kartma gerekmez.
   - **Temsilcileri:** Takviyeli √∂ƒürenme (RL), adaptasyon mekanizmalarƒ±.
   - **√ñzellik:** Ger√ßek d√ºnya etkile≈üimi ve deneyimsellik odaklƒ±dƒ±r.

### **Kƒ±saca:**
- **Symbolism:** Kurallar ve sembollerle √ßalƒ±≈üƒ±r (geleneksel AI).
- **Connectionism:** Sinir aƒülarƒ± ve derin √∂ƒürenmeyi √∂ne √ßƒ±karƒ±r.
- **Behaviorism:** Algƒ±-eylem d√∂ng√ºs√º ve √ßevresel etkile≈üime dayalƒ±dƒ±r.

Bu d√º≈ü√ºnce okullarƒ±, AI'nin farklƒ± yakla≈üƒ±mlarƒ±nƒ± temellendirir.

## **Ana AI Teknolojileri (Huawei Confidential belgesine g√∂re):**

1. **Bilgisayar G√∂r√ºm√º (Computer Vision):**
   - Makinelerin g√∂rsel verileri (resimler, videolar) "g√∂rerek" anlamasƒ±.
   - **√ñrnekler:** Nesne tespiti, y√ºz tanƒ±ma, otomatik s√ºr√º≈ü.

2. **Konu≈üma ƒ∞≈üleme (Speech Processing):**
   - Ses sinyallerini i≈üleme, tanƒ±ma ve sentezleme.
   - **√ñrnekler:** Konu≈üma tanƒ±ma (Siri), makineye sesle komut verme.

3. **Doƒüal Dil ƒ∞≈üleme (NLP - Natural Language Processing):**
   - Bilgisayarlarƒ±n insani dil (metin/konu≈üma) anlamasƒ±.
   - **√ñrnekler:** Chatbot'lar, dil √ßevirisi, duygu analiz.

### **Ek Bilgi:**
- **Uygulama Alanlarƒ±:** Akƒ±llƒ± kentler, saƒülƒ±k, sanayi 4.0, t√ºketici deneyimi.
- **Teknolojik Temeller:** Yapay sinir aƒülarƒ± (DL), takviyeli √∂ƒürenme (RL), sembolik AI.

## **Huawei'nin Full-Stack AI Stratejisi Adƒ±mlarƒ± Kƒ±saca:**

1. **Chip Enablement (Ascend Serisi):**
   - **Ascend** (NPU tabanlƒ± i≈ülemciler) ve **CANN** (optimizasyon k√ºt√ºphanesi) ile donanƒ±m desteƒüi saƒülar.
   - **Ascend-Max, Ascend-Mini, Ascend-Tiny** gibi farklƒ± performans seviyeleri i√ßin √ß√∂z√ºmler sunar.

2. **Framework (MindSpore):**
   - **MindSpore**, device-edge-cloud i√ßin uyumlu eƒüitim/√ßƒ±karma (training/inference) √ßer√ßevesi sunar.
   - **Automatik paralellik** ve CPU/GPU/Ascend i≈ülemcilerinin desteklenmesi ile esneklik saƒülar.

3. **Application Enablement (ModelArts):**
   - **ModelArts**, end-to-end AI hizmetleri (model eƒüitimi, deploy, optimize) i√ßin platform sunar.
   - √ñn-integrasyonlu √ß√∂z√ºmler ve senaryo √∂zg√º API'ler saƒülar.

### **√ñzet:**
Huawei, **Ascend i≈ülemcileri** ile donanƒ±m, **MindSpore** ile yapƒ±sal √ßer√ßeve ve **ModelArts** ile uygulama d√ºzeyinde b√ºt√ºnle≈ütirme ile **t√ºm senaryolar (device-edge-cloud)** i√ßin AI √ß√∂z√ºmleri sunar.

![alt text](notes-images/image-28.png)

## Rule based ve ML
ML (Machine Learning) ve rule-based algoritmalar arasƒ±nda temel farklar ≈üunlardƒ±r:

1. **√ñƒürenme Y√∂ntemi:**
   - **Rule-based:** Kurallar kullanƒ±cƒ± tarafƒ±ndan elle tanƒ±mlanƒ±r (√∂rneƒüin, "eƒüer ya≈üƒ± 18'den k√º√ß√ºkse izin ver").
   - **ML:** Makineler, eƒüitim verilerinden otomatik olarak karar kurallarƒ±nƒ± √∂ƒürenir (√∂rneƒüin, belirli √∂zelliklere g√∂re sƒ±nƒ±flandƒ±rma).

2. **Veri Tabanlƒ±lƒ±k:**
   - **Rule-based:** Sabit kurallar kullanƒ±r ve veriye baƒüƒ±mlƒ± deƒüildir.
   - **ML:** Veriden √∂ƒürenme yapar ve modelin performansƒ± eƒüitim verilerine baƒülƒ±dƒ±r.

3. **Kararlƒ±lƒ±k ve Esneklik:**
   - **Rule-based:** Kurallar deƒüi≈ümez olabilir, ancak karma≈üƒ±k senaryolarda yetersiz kalabilir.
   - **ML:** Kompleks verileri modellemek i√ßin daha esnek olan modeller olu≈üturur.

4. **Uygulama Alanƒ±:**
   - **Rule-based:** Basit ve belirli kurallar gerektiren problemlere (√∂rneƒüin, trafik ƒ±≈üƒ±klarƒ±) uygundur.
   - **ML:** B√ºy√ºk veri setlerinde gizli kurallarƒ± ke≈üfetmek istenen durumlarda (√∂rneƒüin, spam tespiti) daha uygun olur.

## ML √ß√∂z√ºmleri

Machine Learning (ML) temel olarak a≈üaƒüƒ±daki t√ºrde problemlere √ß√∂z√ºm sunar:

1. **Sƒ±nƒ±flandƒ±rma (Classification):**
   - Veri giri≈üini belirli kategorilere atamak.
   - √ñrnek: G√∂r√ºnt√º sƒ±nƒ±flandƒ±rma (meme kanseri tespiti).

2. **Regresyon (Regression):**
   - Girdi verileri i√ßin s√ºrekli √ßƒ±ktƒ± tahmin etmek.
   - √ñrnek: Emlak fiyatƒ± tahmini (ev metrekaresi verisi kullanƒ±larak).

3. **K√ºmeleme (Clustering):**
   - Etiketlenmemi≈ü veri setlerini i√ßsel benzerliklere g√∂re kategorilere ayƒ±rmak.
   - √ñrnek: Kullanƒ±cƒ± profil y√∂netimi veya g√∂r√ºnt√º arama.

## ML kategorileri
Machine Learning (ML) sƒ±nƒ±flarƒ± temel olarak **√º√ß ana kategoriye** ayrƒ±lƒ±r:

1. **Supervised Learning (G√∂zlemlenen √ñƒürenme):**
   - Etiketlenmi≈ü veriler kullanarak model eƒüitilir.
   - **Sƒ±nƒ±flandƒ±rma** (√∂rneƒüin, spam tespiti) ve **regresyon** (√∂rneƒüin, fiyat tahmini) problemlerinin √ß√∂z√ºm√º i√ßin kullanƒ±lƒ±r.
   - *√ñrnek:* SVM (Support Vector Machine), Linear Regression.

2. **Unsupervised Learning (G√∂zlemsiz √ñƒürenme):**
   - Etiketsiz verilerden modeller √∂ƒürenir.
   - **K√ºmeleme** (√∂rneƒüin, m√º≈üteri segmentasyonu) ve **boyut indirgeme** (√∂rneƒüin, PCA) gibi problemler i√ßin uygundur.
   - *√ñrnek:* K-Means, DBSCAN.

3. **Reinforcement Learning (Takviyeli √ñƒürenme):**
   - Eylemlerin sonucuna g√∂re √∂d√ºller alarak √∂ƒürenme yapar.
   - Oyun oynama robotlar veya √∂neri sistemlerinde kullanƒ±lƒ±r.
   - *√ñrnek:* Q-Learning.

**Ek Not:**
- **Ensemble Y√∂ntemleri** (√∂rneƒüin, Random Forest, GBDT) veya **Yarƒ±-G√∂zlemlenen √ñƒürenme** (√∂rneƒüin, Semi-Supervised Learning) gibi alt kategoriler de mevcuttur.

### Supervised i√ßin:
### **1. Sƒ±nƒ±flandƒ±rma (Classification) √ñrnekleri:**
- "*Bir e-posta spam (i≈ülevsiz) mi yoksa normal (i≈ülevsel) mi?*"
- "*Bir hasta meme kanseri riski altƒ±nda mƒ±?*"
- "*Bir m√º≈üteri kredi kartƒ± ba≈üvurusu kabul edilecek mi?*"

### **2. Regresyon (Regression) √ñrnekleri:**
- "*Bu evin fiyatƒ± ne kadar olacaktƒ±r?*"
  - **Ama√ß:** Ev metrekaresi, konum gibi √∂zelliklere g√∂re fiyat tahmini.

- "*Bu arzandaki malƒ±n satƒ±≈ü miktarƒ± ne olacak?*"
  - **Ama√ß:** Satƒ±≈ü verilerini kullanarak talebi tahmin etmek.
  
---

### Unsupervised i√ßin:
### **Clustering (K√ºmeleme) ƒ∞√ßin √ñrnek Sorular:**

- *"Hangi m√º≈üteri gruplarƒ± benzer alƒ±≈üveri≈ü davranƒ±≈ülarƒ± sergiliyor?"*
   - **Ama√ß:** M√º≈üterileri satƒ±n aldƒ±klarƒ± √ºr√ºnler, harcadƒ±klarƒ± para miktarƒ± ve frekans gibi √∂zelliklere g√∂re segmentlere ayƒ±rmak.


### **Feature Selection (√ñzellik Se√ßimi) √áe≈üitleri:**

1. **Filter Methods (Filtre Y√∂ntemleri):**
   - √ñzelliklerin hedef deƒüi≈üken ile ili≈ükisini istatistiksel √∂l√ß√ºtler kullanarak deƒüerlendirir.
   - *√ñrnekler:* Chi-Square, ANOVA, Mutual Information.
   - **Avantaj:** Hƒ±zlƒ± ve verimlidir.

2. **Wrapper Methods (Sarƒ±layƒ±cƒ± Y√∂ntemleri):**
   - Farklƒ± modellerin performansƒ±nƒ± kullanarak en iyi √∂zellik k√ºmesini belirler.
   - *√ñrnekler:* Forward Selection, Backward Elimination.
   - **Avantaj:** Model performansƒ±na odaklanƒ±r.

3. **Embedded Methods (G√∂m√ºl√º Y√∂ntemler):**
   - Model eƒüitimi sƒ±rasƒ±nda √∂zelliƒüi se√ßer.
   - *√ñrnekler:* **Lasso Regression (L1 regularization)**, Ridge Regression (L2 regularization).
   - **Avantaj:** Model karma≈üƒ±klƒ±ƒüƒ±nƒ± azaltƒ±r.

**√ñzet:**
- **Filtreler** hƒ±zlƒ± se√ßer,
- **Sarƒ±layƒ±cƒ±lar** model performansƒ±na g√∂re se√ßer,
- **G√∂m√ºl√º y√∂ntemler** model eƒüitimiyle birlikte se√ßer.

---

### **Model Validity (Model Doƒüruluƒüu) √ñzeti:**

1. **Genelle≈ütirme Yeteneƒüi (Generalization):**
   - Modelin **yeni verilerde** de iyi performans g√∂sterme √∂zelliƒüidir.
   - **Genelle≈ütirme hata** (generalization error): Yeni verilerde yapƒ±lan hatalar.
   - **Eƒüitim hatasƒ±** (training error): Eƒüitim verilerinde yapƒ±lan hatalar.

2. **Underfitting (Yetersiz √ñƒürenme):**
   - Model verileri iyi √∂ƒürenemez (y√ºksek eƒüitim hata).
   - *√ñrnek:* √áok basit bir model kullanmak.

3. **Overfitting (A≈üƒ±r √ñƒürenme):**
   - Model eƒüitim verilerini m√ºkemmel √∂ƒürenir ama yeni verilerde ba≈üarƒ±sƒ±z olur (y√ºksek genelle≈ütirme hata).
   - *√ñrnek:* √áok karma≈üƒ±k bir model kullanmak.

4. **Hedef ve Hipotez Fonksiyonlarƒ±:**
   - **Target function (f):** Ger√ßek d√ºnyadaki ideal fonksiyon (bilinmez).
   - **Hypothesis function (g):** Modelin tahmin ettiƒüi fonksiyon (f'nin yakla≈üƒ±ƒüƒ±).

**Ana Amacƒ±:** **D√º≈ü√ºk genelle≈ütirme hata** saƒülayan modeller elde etmek.

---

### **Bias ve Variance (Kƒ±saca):**

1. **Bias (Y√∂nelim/Sapma):**
   - Modelin **ger√ßek fonksiyonu** (target function) taklit etme yeteneƒüinin yetersizliƒüi.
   - **√áok y√ºksek bias** (underfitting): Model basit ve hatalƒ± tahminler yapar.
   - *√ñrnek:* Linear regression kullanƒ±p bir kare fonksiyonu tahmin etmeye √ßalƒ±≈ümak.

2. **Variance (Varyans):**
   - Modelin **eƒüitim verilerindeki k√º√ß√ºk deƒüi≈üimlere** g√∂re √ßok hassas olma durumu.
   - **√áok y√ºksek variance** (overfitting): Model eƒüitim verilerini kendisi yerine t√ºm veri k√ºmesini √∂ƒürenir.
   - *√ñrnek:* 1000'nin √ºzerinde decision tree d√ºƒü√ºm√º kullanmak.

3. **Ideal Model:**
   **Bias ve Variance'ƒ±n dengeli olmasƒ±na** ihtiya√ß vardƒ±r.
   - **D√º≈ü√ºk bias & d√º≈ü√ºk variance** ‚Üí ƒ∞yi model (√∂rneƒüin: Random Forest).
   - **D√º≈ü√ºk bias & y√ºksek variance** ‚Üí Overfitting.
   - **Y√ºksek bias & d√º≈ü√ºk variance** ‚Üí Underfitting.
   - **Y√ºksek bias & y√ºksek variance** ‚Üí K√∂t√º model.

*(Context'teki "Variance and Bias" b√∂l√ºm√ºnden bilgiler kullanƒ±lmƒ±≈ütƒ±r.)*

**√ñzet:**
- **Bias** = Modelin hatalƒ± genel kurallar olu≈üturmasƒ±.
- **Variance** = Modelin verilerdeki g√ºr√ºlt√ºye √ßok duyarlƒ± olmasƒ±.
- **Hedef:** **Dengeyi** bulmak! (Bias-Variance Tradeoff)
  
---

### **Gradient Descent (G√∂r√ºn√ºm Deseni) Y√∂ntemleri:**

1. **Batch Gradient Descent (BGD - Toplu G√∂r√ºn√ºm Deseni):**
   - **T√ºm eƒüitim verilerini** kullanarak aƒüƒ±rlƒ±klarƒ± g√ºnceller.
   - **Daha stabil** ama **yava≈ü** √ßalƒ±≈üƒ±r (her seferinde t√ºm veri setini i≈üler).
   - **Uygulama:** B√ºy√ºk veriler i√ßin yetersiz (hesaplama maliyeti y√ºksek).      

2. **Stochastic Gradient Descent (SGD - Rastgele G√∂r√ºn√ºm Deseni):**
   - **Her seferinde tek tek veri √∂rneklerini** kullanƒ±r.
   - **Hƒ±zlƒ±** ancak **g√ºr√ºlt√ºl√º** (genellikle optimal noktaya titreme ile ula≈üƒ±r).
   - **Uygulama:** B√ºy√ºk verilerde tercih edilir (mini-batch SGD ile optimizasyonlar yapƒ±lƒ±r).

3. **Mini-Batch Gradient Descent (MBGD - K√º√ß√ºk Toplu G√∂r√ºn√ºm Deseni):**
   - **BGD ve SGD arasƒ±nda bir dengedir.** Her seferinde **bir grup veri (mini-batch)** kullanƒ±r.
   - **Denge:** Hƒ±zlƒ± ve daha stabilize √ßalƒ±≈üƒ±r (SGD'den daha az g√ºr√ºlt√º, BGD'den daha hƒ±zlƒ±).
   - **Uygulama:** Genellikle **MBGD tercih edilir** (√∂rneƒüin, 32-256 √∂rneklik mini-batch'lar).

### **√ñzet:**
| Y√∂ntem          | Veri Kullanƒ±mƒ±       | Hƒ±zlƒ±lƒ±k | Stabilite | Uygulama |
|-----------------|----------------------|----------|-----------|----------|
| **BGD**         | T√ºm veriler          | Yava≈ü    | Y√ºksek    | K√º√ß√ºk veriler |
| **SGD**         | Tek tek √∂rnekler     | Hƒ±zlƒ±    | D√º≈ü√ºk     | B√ºy√ºk veriler |
| **MBGD**        | Mini-batch'lar       | Orta     | Orta       | Genel kullanƒ±m |

---

### **Hiperparametreler ve Metodlarƒ± (√ñzet):**

#### **1. Hiperparametreler Nedir?**
- **Modelin dƒ±≈üarƒ±dan ayarlanabilen** ayarlarƒ±dƒ±r.
- **Parametrelerden farklƒ±dƒ±r:** Parametreler modelin eƒüitim sƒ±rasƒ±nda √∂ƒürenilen deƒüerlerdir (√∂rneƒüin, linear regression'deki katsayƒ±lar).
- **√ñrnekler:**
  - **Lasso/Ridge Regression:** `Œª` (landa, ceza terimi).
  - **Neural Network:** √ñƒürenme hƒ±zƒ± (learning rate), batch size, katman sayƒ±sƒ±.
  - **SVM:** `C` (ceza parametresi) ve `œÉ` (√ßevreleme fonksiyonu).
  - **Random Forest:** Aƒüa√ß sayƒ±sƒ±.
  - **KNN:** `k` (en yakƒ±n kom≈üu sayƒ±sƒ±).

#### **2. Hiperparametre Optimizasyon Metodlarƒ±:**
- **1. Grid Search (ƒ∞zleme Aramasƒ±):**
  - **T√ºm m√ºmk√ºn kombinasyonlarƒ±** deneyerek en iyi hiperparametreyi bulur.
  - **Avantaj:** Esnek ve sistematik.
  - **Dezavantaj:** **√áok zaman alƒ±cƒ±** (sayƒ±larƒ± fazla olduƒüunda).
  - **Uygulama:** KNN, SVM gibi basit modellerde kullanƒ±lƒ±r.

- **2. Random Search (Rastgele Arama):**
  - **Rastgele se√ßilen hiperparametre kombinasyonlarƒ±** denenir.
  - **Avantaj:** Grid search'e g√∂re daha hƒ±zlƒ± ve genellikle daha iyi sonu√ßlar verir.

- **3. Bayesian Optimization (Baysiyen Optimizasyon):**
  - **Diƒüer deneyimlerden √∂ƒürenerek** daha iyi adaylar se√ßer.
  - **Avantaj:** Daha verimli (az sayƒ±da deneme ile iyi sonu√ßlar verir).

- **4. Gradient-Based Optimization (Gradient Tabanlƒ± Optimizasyon):**
  - **Hiperparametrelerin kayƒ±p fonksiyonuna g√∂re** g√ºncellenmesi.
  - **Uygulama:** Derin √∂ƒürenme modellerinde (√∂rneƒüin, learning rate optimizasyonu).

- **Cross Validation**, modelin performansƒ±nƒ± doƒürulamak i√ßin verileri **eƒüitim seti** ve **doƒürulama seti** ≈üeklinde b√∂lerek tekrar tekrar test eden bir y√∂ntemdir. Modeli eƒüitim setiyle eƒüitip, doƒürulama setiyle test eder.
- **K-Fold CV**, en yaygƒ±n kullanƒ±lan y√∂ntemdir (√∂rneƒüin, 5-Fold).    - Veriyi **k adet e≈üit par√ßaya** b√∂ler (√∂rneƒüin, k=5).
   - Her seferinde **1 par√ßa doƒürulama seti**, **diƒüer k-1 par√ßa eƒüitim seti** olur.
   - **k adet farklƒ± model** eƒüitilir ve ortalama doƒürulama ba≈üarƒ± oranƒ± hesaplanƒ±r.
- **Amacƒ±:** Overfitting riskini azaltarak modelin genel performansƒ±nƒ± artƒ±rmaktƒ±r.

---

## G√∂revler:

**Classification:** Algoritma genel olarak bi f fonksiyonundan sƒ±nƒ±flara √ßƒ±ktƒ± veren bir fonksiyondur.  f: R^n -> (1,2,..,k)

**Regression:** Model, veriye g√∂re bir √ßƒ±ktƒ± tahmin etmeye √ßalƒ±≈üƒ±r. Algoritmasƒ± genel olarak bir √ßƒ±ktƒ± fonksiyonu verir. f: R^n -> R   
√ñrn. sigorta √ºcretini tahmin etmeye √ßalƒ±≈üan bir model. 

**Clustering:** Etiketsiz, b√ºy√ºk bir verisetindeki verileri, i√ßerisindeki benzerliƒüe g√∂re bir√ßok gruba b√∂ler. Aynƒ± kategorideki veriler birbirine daha √ßok benzer. √ñrn. image retrieval, user profile management.

Tahmin etme problemleri i√ßin %80-%90 olarak classification ve regression kullanƒ±lƒ±r.

## √áe≈üitleri

**Supervised learning:** Labeled data. Tangible(elle tutulur, somut) ve intangible(soyut) objeleri sƒ±nƒ±fandƒ±rmada kullanƒ±lƒ±r. Obje hakkƒ±nda bilgi vermeden. 

Product recommendatitons, segment customers based on customer data, diognose a disease etc.

**Unsupervised learning:** Unlabeled data. Makineyi b√ºy√ºk deƒüi≈üken verilere maruz bƒ±rakƒ±lƒ±r. Clustering sƒ±klƒ±kla kullanƒ±lƒ±r.

**Semi-supervised learning:** K√º√ß√ºk miktarda labeled verinin doƒürudan √∂ƒürenilmesine yardƒ±mcƒ± olmak i√ßin b√ºy√ºk miktarda unlabeled veri kullanƒ±r. Sƒ±nƒ±f videolarƒ±ndan √∂ƒürencilerin √∂ƒürenme ≈üeklini anlamak vb.

**Reinforcement learning:** Algoritmayƒ± √∂d√ºl ve ceza y√∂ntemine g√∂re eƒüitir. Agent, √ßevresi ile etkile≈üime girer, doƒüru ise √∂d√ºl, yanlƒ±≈ü ise ceza verilir.

## Feature Selection
Filter: Filtre metodlarƒ± modelden baƒüƒ±msƒ±z
Wrapper: Feature subset tanƒ±mlamak i√ßin bir tahmin modeli kullanƒ±r
Embedded: Bu metodlar future selection'u modelin bi yapƒ±sƒ± olarak ele alƒ±r  

Bias: Beklenen/ortalama tahmin deƒüerinin ger√ßek deƒüerden ne kadar farklƒ± olduƒüu

Variance: Tahmin deƒüerinin, her tahminde ortalama deƒüerden ne kadar deƒüi≈ütiƒüi

Dependent variable: tahmin edilecek deƒüer
independent variable: tahmin etmek i√ßin kullanƒ±lacak deƒüer

# Deep Learning Overview

Derin √∂ƒürenme modeli unsupervised feature learning ve feature hierarchy learning'e dayanƒ±r.

Feature selection/extraction kƒ±smƒ±nda daha otomatiktir.

Aktivasyon fonksiyonu:
Sigmoid: Vanishing gradient problem / sigmoid sƒ±fƒ±r olduƒüunda fonksiyon √ßalƒ±≈ümayƒ± durdurur

# Mainstream Frameworks

-> In TensorFlow 2.x, eager execution is enabled by default. ( )(1.0 points)

# ƒ∞kinci Kurs

# Neural Networks Quiz

## True or False
The neuron with computing capability at hidden layer is referred to as the computing unit.(1.0 points) - T

Increasing the number of parameters in layers of convolutional networks without increasing their depth will enhance the ability of our model.(1.0 points) - T

Using a deep model expresses a useful preference over the space of functions the model can learn.(1.0 points) -T

Dataset includes features and samples.A feature is used to describe the characteristics of an object.A sample is object or event with many features.(1.0 points) - T

Semi-supervised learning is on the part of the data has labels and others don't have their labels.(1.0 points) - T

If points on the line connected between any two points in the set are also in the set, the set is a convex set.(1.0 points) -T

The backpropagation algorithm can be used by the gradient descent optimization algorithm to adjust neural network weights.(1.0 points) -T

## Single Choice
Follow these operation, Which one is not right?(1.0 points) (A)
- A. BGD is more sensitive to noise than SGD.
- B. Mini-batch gradient descent (MBGD) is used small batch size training dataset to update the weights.
- C. All training data is used for each update to minimize the loss function in BGD.
- D. Only one sample point is considered during each update in SGD.

Which of the following methods can be used to implement backpropagation?(1.0 points) (A)
- A. Chain rule
- B. Computational graph
- C. Cost function
- D. Higher order differential

## Multiple Choices
For AI tasks. We can divide a task into four classes. Which calss is AI task.(1.0 points) (all)
- A. Supervised learning algorithms
- B. Semi-supervisor learning
- C. Reinforcement learning
- D. Unsupervised learning algorithms

Follow these operation, Which one is not right?(1.0 points) (all)
- A. ReLU has a derivative function and allows for backpropagation.
- B. ELU has an extra alpha constant which should be positive number.
- C. Softmax function calculates the probabilities of the event over 'n' different events.
- D. The activation function should be non-linear.

# Image Processing

Sampling: G√∂r√ºnt√ºdeki piksellerin sayƒ±sƒ±nƒ±n azaltƒ±lmasƒ±.
Quantization: G√∂r√ºnt√ºdeki piksellerin renk sayƒ±sƒ±nƒ±n azaltƒ±lmasƒ±. Mesela grayscale'e √ßevirmek.  
8-bits: 256 renk  
2-bits: 4 renk

![alt text](image.png)

# Image Representation

![alt text](image-1.png)
buradaki matris'e channel denir.

![alt text](image-2.png)

RGB: Red, Green, Blue. Ekranda g√∂r√ºnt√ºleri g√∂stermek i√ßin kullanƒ±lƒ±r. 3 channel i√ßerir. 255 0 0  

HSV: Hue, Saturation, Value  
Burada V: 0-1, S: 0-1, H: 0-360 arasƒ±nda deƒüi≈üir.

YUV: Luminance (Y) ve chrominance (U, V) bile≈üenlerinden olu≈üur. Y ile U,V birbirinden ayrƒ±dƒ±r.
Eƒüer bir g√∂r√ºnt√º sadece Y bile≈üenini i√ßeriyorsa, bu g√∂r√ºnt√º grayscale olarak adlandƒ±rƒ±lƒ±r.

CMYK: Cyan, Magenta, Yellow, Black. CMYK renk uzayƒ±, baskƒ± end√ºstrisinde yaygƒ±n olarak kullanƒ±lƒ±r. CMY renk uzayƒ± ise RGB'nin tersine dayanƒ±r.

![alt text](image-3.png)
Position feature: G√∂r√ºnt√ºdeki piksellerin konumunu temsil eder.

Connected Region: G√∂r√ºnt√ºdeki piksellerin birbirine baƒülƒ± olduƒüu b√∂lgeleri temsil eder.

# Image Transformation

Coordinate Transformation: G√∂r√ºnt√ºdeki piksellerin konumunu deƒüi≈ütirmek i√ßin kullanƒ±lƒ±r. √ñrneƒüin, bir g√∂r√ºnt√ºy√º d√∂nd√ºrmek veya √∂l√ßeklendirmek i√ßin kullanƒ±labilir. p' = Ap + b ≈üeklinde ifade edilir.

Transpose: Matrisin satƒ±rlarƒ±nƒ± s√ºtunlara, s√ºtunlarƒ±nƒ± satƒ±rlara d√∂n√º≈üt√ºr√ºr. √ñrneƒüin, A^T.

![alt text](image-4.png)
Mirror: G√∂r√ºnt√ºy√º yatay veya dikey olarak yansƒ±tƒ±r. √ñrneƒüin, A^M.

![alt text](image-5.png)
Rotate: G√∂r√ºnt√ºy√º belirli bir a√ßƒ±yla d√∂nd√ºr√ºr. √ñrneƒüin, A^R.

![alt text](image-6.png)
Zooming: G√∂r√ºnt√ºy√º belirli bir √∂l√ßekle b√ºy√ºt√ºr veya k√º√ß√ºlt√ºr. √ñrneƒüin, A^Z.

![alt text](image-7.png)
Interpolation: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini tahmin etmek i√ßin kullanƒ±lƒ±r.
G√∂r√ºnt√ºdeki (x,y) integer olmalƒ±dƒ±r, ama transformasyon sonrasƒ± (x,y) float olabilir. Bu durumda interpolation kullanƒ±lƒ±r.

![alt text](image-8.png)
Grayscale Transformation-Inversion: G√∂r√ºnt√ºy√º grayscale'e d√∂n√º≈üt√ºr√ºr. Inversi ise g√∂r√ºnt√ºn√ºn renklerini tersine √ßevirir.

![alt text](image-9.png)
Grayscale Transformation-Contrast Enhancement: G√∂r√ºnt√ºn√ºn kontrastƒ±nƒ± artƒ±rƒ±r. Bu, g√∂r√ºnt√ºdeki parlaklƒ±k farklarƒ±nƒ± artƒ±rarak daha net bir g√∂r√ºnt√º elde etmeyi saƒülar.

![alt text](image-10.png)
Contrast compression: G√∂r√ºnt√ºn√ºn kontrastƒ±nƒ± azaltƒ±r. Bu, g√∂r√ºnt√ºdeki parlaklƒ±k farklarƒ±nƒ± azaltarak daha yumu≈üak bir g√∂r√ºnt√º elde etmeyi saƒülar.

![alt text](image-11.png)
Gamma correction: G√∂r√ºnt√ºn√ºn parlaklƒ±ƒüƒ±nƒ± ayarlamak i√ßin kullanƒ±lƒ±r. Gamma deƒüeri, g√∂r√ºnt√ºn√ºn parlaklƒ±k d√ºzeyini kontrol eder. Gamma deƒüeri 1'den b√ºy√ºkse, g√∂r√ºnt√º daha parlak olur; 1'den k√º√ß√ºkse, g√∂r√ºnt√º daha karanlƒ±k olur.

# Histogram

![alt text](image-12.png)
Histogram of a grayscale image: Grayscale g√∂r√ºnt√ºn√ºn histogramƒ±, g√∂r√ºnt√ºdeki piksel deƒüerlerinin daƒüƒ±lƒ±mƒ±nƒ± g√∂sterir. Histogram, g√∂r√ºnt√ºdeki parlaklƒ±k d√ºzeylerini analiz etmek i√ßin kullanƒ±lƒ±r. B√ºt√ºn g√∂r√ºnt√ºdeki piksel deƒüerlerinin sayƒ±sƒ±nƒ± g√∂sterir.

![alt text](image-13.png)
Histogram calculation: Histogram, g√∂r√ºnt√ºdeki piksel deƒüerlerinin daƒüƒ±lƒ±mƒ±nƒ± g√∂sterir. Her piksel deƒüeri i√ßin, o deƒüere sahip piksel sayƒ±sƒ±nƒ± hesaplar.

![alt text](image-14.png)
Histogram equalization: Histogram e≈üitleme, g√∂r√ºnt√ºn√ºn kontrastƒ±nƒ± artƒ±rmak i√ßin kullanƒ±lƒ±r. Bu i≈ülem, g√∂r√ºnt√ºdeki piksel deƒüerlerinin daƒüƒ±lƒ±mƒ±nƒ± daha dengeli hale getirir. 

Histogram e≈üitleme, g√∂r√ºnt√ºdeki d√º≈ü√ºk kontrastlƒ± b√∂lgeleri aydƒ±nlatƒ±r ve y√ºksek kontrastlƒ± b√∂lgeleri karartƒ±r. 

Otomatik olarak hesaplar. API √ßaƒürƒ±sƒ± ile invoke edilir. Generates an image that globally equalized.

![alt text](image-15.png)
Histogram specification: Histogram spesifikasyonu, bir g√∂r√ºnt√ºn√ºn histogramƒ±nƒ± belirli bir hedef histogram ile e≈üle≈ütirmek i√ßin kullanƒ±lƒ±r. Bu i≈ülem, g√∂r√ºnt√ºn√ºn kontrastƒ±nƒ± artƒ±rmak ve belirli bir renk daƒüƒ±lƒ±mƒ±nƒ± elde etmek i√ßin kullanƒ±lƒ±r.

# Filtering

![alt text](image-16.png)
Spatial filtering: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini kom≈üu piksellerin deƒüerlerine g√∂re deƒüi≈ütirir. Bu, g√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak veya kenarlarƒ± vurgulamak i√ßin kullanƒ±lƒ±r.
N neighbours genellikle bir tek sayƒ± olarak se√ßilir. √ñrneƒüin, 3x3, 5x5 gibi.
central pixel - p

![alt text](image-17.png)
Mean filter: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini, kom≈üu piksellerin ortalamasƒ± ile deƒüi≈ütirir. Bu, g√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak i√ßin kullanƒ±lƒ±r. Genellikle blur efektini elde etmek i√ßin kullanƒ±lƒ±r.

Median filter: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini, kom≈üu piksellerin medyanƒ± ile deƒüi≈ütirir. Bu, g√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak i√ßin kullanƒ±lƒ±r. Intensity yi deƒüi≈ütirmez. √ñzellikle tuz ve biber g√ºr√ºlt√ºs√ºn√º azaltmak i√ßin etkilidir.

√áok detaylƒ± g√∂r√ºnt√ºlerde iyi deƒüildir. √á√ºnk√º detaylarƒ± da kaybeder.

![alt text](image-18.png)
Gaussian filter: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini, kom≈üu piksellerin Gaussian daƒüƒ±lƒ±mƒ±na g√∂re deƒüi≈ütirir. Bu, g√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak i√ßin kullanƒ±lƒ±r. Gaussian daƒüƒ±lƒ±mƒ±, merkezi pikselin etrafƒ±ndaki piksellere daha fazla aƒüƒ±rlƒ±k verir.

![alt text](image-19.png)
Sharpening filter: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini, kom≈üu piksellerin deƒüerlerine g√∂re deƒüi≈ütirir. Bu, g√∂r√ºnt√ºdeki kenarlarƒ± vurgulamak i√ßin kullanƒ±lƒ±r. Genellikle blur etkisini azaltmak i√ßin kullanƒ±lƒ±r.
![alt text](image-20.png)
Buradaki operatorler edge detection i√ßin de kullanƒ±lƒ±r.

![alt text](image-21.png)
Affine transformation: G√∂r√ºnt√ºy√º √∂l√ßeklendirme, d√∂nd√ºrme, yansƒ±tma gibi i≈ülemlerle d√∂n√º≈üt√ºr√ºr. Bu, g√∂r√ºnt√ºn√ºn geometrik √∂zelliklerini deƒüi≈ütirmek i√ßin kullanƒ±lƒ±r.

![alt text](image-22.png)
Perspective transformation: G√∂r√ºnt√ºy√º perspektif olarak d√∂n√º≈üt√ºr√ºr. Bu, g√∂r√ºnt√ºn√ºn perspektif √∂zelliklerini deƒüi≈ütirmek i√ßin kullanƒ±lƒ±r. √ñrneƒüin, bir g√∂r√ºnt√ºy√º farklƒ± bir a√ßƒ±dan g√∂rmek i√ßin kullanƒ±labilir.

Nonlinear bir transformation'dƒ±r. 8 tane parametre ile ifade edilir. 4 tane kaynak nokta ve 4 tane hedef nokta ile ifade edilir.

![alt text](image-23.png)
Color image processing: Farklƒ± kanallarƒ± ayrƒ± olarak i≈üler. Sonrasƒ±nda kanallarƒ± birle≈ütirir. Diƒüer bir yol, her bir pikseli multi dimensional ayrƒ± bir vekt√∂r olarak ele alƒ±p, bu vekt√∂rleri ayrƒ± ayrƒ± i≈üler.

![alt text](image-24.png)
Brightness enhancement: G√∂r√ºnt√ºn√ºn parlaklƒ±ƒüƒ±nƒ± artƒ±rmak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki piksel deƒüerlerini belirli bir deƒüere ekleyerek veya √ßƒ±kararak yapƒ±lƒ±r.

![alt text](image-25.png)
Saturation enhancement: G√∂r√ºnt√ºn√ºn doygunluƒüunu artƒ±rmak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki renklerin yoƒüunluƒüunu artƒ±rarak daha canlƒ± renkler elde etmeyi saƒülar.

![alt text](image-26.png)
Hue enhancement: G√∂r√ºnt√ºn√ºn rengini deƒüi≈ütirmek i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki renk tonlarƒ±nƒ± deƒüi≈ütirerek farklƒ± renkler elde etmeyi saƒülar.

![alt text](image-27.png)
Data augmentation tipleri: zooming, strecthing, adding noise, flipping, rotation, translation, sheaaring, contrast adjustment, channel transformation ..

# Image Recognition

Precision'u genellikle proportion (oran)'a bakmak istediƒüimizde kullanƒ±rƒ±z.
Recall rate i medikal alanda sƒ±k kullanƒ±rƒ±z. √á√ºnk√º olabildiƒüince doƒüru tahmin yapmak isteriz.

![alt text](image-29.png)
IoU (Intersection over Union): ƒ∞ki nesnenin kesi≈üim alanƒ±nƒ±n, birle≈üim alanƒ±na oranƒ±nƒ± √∂l√ßer. Genellikle nesne tespiti ve segmentasyon problemlerinde kullanƒ±lƒ±r.

![alt text](image-30.png)
Coonected-region segmentation: G√∂r√ºnt√ºdeki piksellerin birbirine baƒülƒ± olduƒüu b√∂lgeleri ayƒ±rmak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki nesneleri veya b√∂lgeleri tanƒ±mlamak i√ßin kullanƒ±lƒ±r. 

Binary image e √ßevirir ve bo≈üluklarƒ± doldurur. b√∂ylece g√∂r√ºnt√ºdeki nesneleri ayƒ±rƒ±r.

![alt text](image-31.png)

![alt text](image-32.png)
Object detection only needs to detect the object at a certain approximate position, while image segmentation needs to segment the target along the boundary of the target area.

U-net, deeplab

![alt text](image-33.png)
Dice coefficient: ƒ∞ki k√ºmenin benzerliƒüini √∂l√ßmek i√ßin kullanƒ±lƒ±r. Genellikle g√∂r√ºnt√º segmentasyonu problemlerinde kullanƒ±lƒ±r. 0 ile 1 arasƒ±nda bir deƒüer alƒ±r. 1, iki k√ºmenin tamamen aynƒ± olduƒüunu g√∂sterir.

![alt text](image-34.png)
Object tracking: Nesnenin hareketini takip etmek i√ßin kullanƒ±lƒ±r.

![alt text](image-35.png)
Character recognition: G√∂r√ºnt√ºdeki karakterleri tanƒ±mak i√ßin kullanƒ±lƒ±r. Bunlarƒ± d√ºzenlenebilir hale getirir. Kelime kelime tanƒ±r.

Genellikle OCR (Optical Character Recognition) sistemlerinde kullanƒ±lƒ±r.

Adƒ±mlarƒ±:
- Image processing
- Binarization
- Character segmentation

![alt text](image-36.png)
![alt text](image-37.png)

# Feature Extraction

![alt text](image-38.png)

Downsampling: G√∂r√ºnt√ºdeki piksellerin sayƒ±sƒ±nƒ± azaltmak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºn√ºn boyutunu k√º√ß√ºltmek ve i≈ülem s√ºresini azaltmak i√ßin yapƒ±lƒ±r.

Region of Interest (ROI): G√∂r√ºnt√ºdeki belirli bir b√∂lgeyi se√ßmek i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºn√ºn belirli bir b√∂l√ºm√ºn√º analiz etmek veya i≈ülemek i√ßin yapƒ±lƒ±r.

Feature descriptor: G√∂r√ºnt√ºdeki belirli √∂zellikleri tanƒ±mlamak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºn√ºn belirli bir b√∂l√ºm√ºndeki √∂zellikleri analiz etmek veya i≈ülemek i√ßin yapƒ±lƒ±r.

![alt text](image-40.png)

Thresholding: G√∂r√ºnt√ºdeki piksellerin deƒüerlerini belirli bir e≈üik deƒüerine g√∂re deƒüi≈ütirir. Bu, g√∂r√ºnt√ºy√º ikili (binary) hale getirmek i√ßin kullanƒ±lƒ±r. Genellikle siyah ve beyaz renkleri ayƒ±rmak i√ßin kullanƒ±lƒ±r.

![alt text](image-39.png)

Binarization: G√∂r√ºnt√ºy√º ikili (binary) hale getirmek i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºy√º siyah ve beyaz renklerle temsil etmek i√ßin yapƒ±lƒ±r. Genellikle g√∂r√ºnt√ºdeki nesneleri ayƒ±rmak i√ßin kullanƒ±lƒ±r.

![alt text](image-41.png)

Bimodal histogram: G√∂r√ºnt√ºdeki piksel deƒüerlerinin iki farklƒ± tepe noktasƒ±na sahip olduƒüu bir histogramdƒ±r. Bu, g√∂r√ºnt√ºdeki nesneleri ayƒ±rmak i√ßin kullanƒ±lƒ±r.

If the histogram of an image is flat or has multiple peaks, the threshold cannot be calculated on the histogram.

All possible thresholds need to traversed and the segmentation threshold with the largest intra-class (sƒ±nƒ±f i√ßi) is selected as optimal threshold.

# Morphological Operations

![alt text](image-42.png)

![alt text](image-43.png)
Dilation: G√∂r√ºnt√ºdeki √∂n plan b√∂lgesini geni≈ületmek i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki nesnelerin boyutunu artƒ±rmak veya bo≈üluklarƒ± doldurmak i√ßin yapƒ±lƒ±r.

Bir k√ºmenin (A) bir yapƒ±sal eleman (B) ile genle≈ütirilmesi (dilation), A‚Äôdaki √∂n plan b√∂lgesinin B‚Äônin ≈üekli doƒürultusunda geni≈ületilmesi i≈ülemidir.

Buradaki merkez nokta B'dir, B'yi A'nƒ±n √ºzerine kaydƒ±rƒ±yoruz. 

![alt text](image-44.png)
Erosion: G√∂r√ºnt√ºdeki √∂n plan b√∂lgesini daraltmak i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºdeki nesnelerin boyutunu azaltmak veya g√ºr√ºlt√ºy√º azaltmak i√ßin yapƒ±lƒ±r.

Bir k√ºmenin (A) bir yapƒ±sal eleman (B) ile erozyonu, A‚Äôdaki √∂n plan b√∂lgesinin B‚Äônin ≈üekli doƒürultusunda daraltƒ±lmasƒ± i≈ülemidir.

Burada merkez nokta B'dir, B'yi A'nƒ±n √ºzerine kaydƒ±rƒ±yoruz. Eƒüer A, B'yi kapsamƒ±yorsa o b√∂lge daraltƒ±lƒ±r.

Erosion i≈ülemi, g√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak ve nesnelerin kenarlarƒ±nƒ± netle≈ütirmek i√ßin kullanƒ±lƒ±r.

![alt text](image-45.png)
Opening: Erosion -> Dilation i≈ülemi. G√∂r√ºnt√ºdeki g√ºr√ºlt√ºy√º azaltmak ve nesnelerin kenarlarƒ±nƒ± netle≈ütirmek i√ßin kullanƒ±lƒ±r.

![alt text](image-46.png)
Closing: Dilation -> Erosion i≈ülemi. G√∂r√ºnt√ºdeki bo≈üluklarƒ± doldurmak ve nesnelerin kenarlarƒ±nƒ± netle≈ütirmek i√ßin kullanƒ±lƒ±r.

![alt text](image-47.png)
Sliding window: G√∂r√ºnt√ºdeki pikselleri kaydƒ±rarak belirli bir b√∂lgeyi se√ßmek i√ßin kullanƒ±lƒ±r. Bu, g√∂r√ºnt√ºn√ºn belirli bir b√∂l√ºm√ºn√º analiz etmek veya i≈ülemek i√ßin yapƒ±lƒ±r.

√áok sƒ±k kullanƒ±lƒ±r. Her seferinde g√∂r√ºnt√ºn√ºn tamamƒ±nƒ± i≈ülemeyiz. 

![alt text](image-48.png)
Template matching: G√∂r√ºnt√ºdeki belirli bir deseni veya nesneyi bulmak i√ßin kullanƒ±lƒ±r.

When the distance is shorter than the threshold, the template is matched.

# Feature Descriptor

![alt text](image-49.png)

![alt text](image-50.png)
HOG (Histogram of Oriented Gradients): Genellikle nesne tespiti ve g√∂r√ºnt√º sƒ±nƒ±flandƒ±rma problemlerinde kullanƒ±lƒ±r. G√∂r√ºnt√ºdeki kenarlarƒ± ve y√∂nleri analiz eder.

G√∂r√ºnt√º, farklƒ± h√ºcrelere b√∂l√ºn√ºr. √ñrneƒüin, bir h√ºcre 18 sector'e b√∂l√ºnebilir ve her bir sekt√∂r√ºn derecesi 20 olabilir. Bu yolla, simetrik sekt√∂rleri gruplayarak 9 tane orientation elde edebiliriz.

√ñrneƒüin: 80x64 pikselli bir g√∂r√ºnt√º, 8x8 pikselli bir h√ºcreye b√∂l√ºnebilir. H√ºcredeki her bir piksel i√ßin gradient hesaplanƒ±r. Then collect the weights votes of gradient orientation over spatial cells.

![alt text](image-51.png)
Her bir pedestrian (yaya) i√ßin √∂zel bir HOG √∂zelliƒüi vardƒ±r. Bu yayalarƒ±n √∂zellikle farklƒ±dƒ±r, yani feature descriptor'larƒ± farklƒ±dƒ±r. Hedef g√∂r√ºnt√ºdeki yayalarƒ± tanƒ±mlamak i√ßin bu template'ler kullanƒ±labilir.

![alt text](image-52.png)
LBP (Local Binary Patterns): G√∂r√ºnt√ºdeki piksellerin kom≈üu piksellerle kar≈üƒ±la≈ütƒ±rƒ±lmasƒ±yla olu≈üturulan bir √∂zellik tanƒ±mlayƒ±cƒ±dƒ±r. Genellikle y√ºz tanƒ±ma ve g√∂r√ºnt√º sƒ±nƒ±flandƒ±rma problemlerinde kullanƒ±lƒ±r.

Kom≈üu piksel, merkez piksel deƒüerinden b√ºy√ºkse, 1, k√º√ß√ºkse 0 olarak i≈üaretlenir. Bu ≈üekilde, her bir piksel i√ßin bir ikili kod olu≈üturulur.

LBP, g√∂r√ºnt√ºdeki merkez piksel ve kom≈üu piksellerin ili≈ükisini tanƒ±mladƒ±ƒüƒ±ndan dolayƒ± g√∂r√ºnt√ºdeki texture (dokusal) √∂zellikleri yakalar.

![alt text](image-53.png)
Haar like features: Bir feature extraction structure'dƒ±r. Basit yapƒ±larƒ± tanƒ±mlar (A, B, C, D). Farklƒ± Haar √∂zellikle farklƒ± alanlarla e≈üle≈üir. Daha fazla √∂zellik ile g√∂r√ºnt√ºn√ºn karakterini anlayabiliriz. Y√ºz tanƒ±mada sƒ±k kullanƒ±lƒ±r.

# CNN

![alt text](image-54.png)

![alt text](image-55.png)
Buradaki convolusion form√ºlleri template match gibi davranƒ±r.

![alt text](image-56.png)
![alt text](image-57.png)
![alt text](image-58.png)
Convolution kernel, bir sliding window'dur. ≈ûekildeki gibi kaydƒ±rƒ±lƒ±r. Her kaydƒ±rmada bir konvol√ºsyon i≈ülemi yapƒ±lƒ±r.

![alt text](image-59.png)
Buradaki filter w0 ve filter w1, birer convolution kernel'dƒ±r. Her bir kernel, g√∂r√ºnt√ºdeki farklƒ± √∂zellikleri yakalar. √ñrneƒüin, w0 yatay kenarlarƒ±, w1 dikey kenarlarƒ± yakalayabilir.

Input volume matrisleri ise, g√∂r√ºnt√ºn√ºn 3 channel'ƒ±nƒ± temsil eder. 3 channel'lƒ± bir g√∂r√ºnt√ºde, her bir channel i√ßin ayrƒ± bir konvol√ºsyon i≈ülemi yapƒ±lƒ±r. Bu, g√∂r√ºnt√ºn√ºn farklƒ± renk kanallarƒ±ndaki √∂zellikleri yakalamak i√ßin kullanƒ±lƒ±r.

Sonrasƒ±nda bias eklenir ve bunun sonucu, feature map'teki yerine yazƒ±lƒ±r. (Output volume kƒ±smƒ±)

# Convolutional Consepts
![alt text](image-60.png)

Convolution kernel: Extracts features from the input image. Ka√ß tane kernel olduƒüu, katmanƒ±n performansƒ±nƒ± etkiler.

Kernel size: W x H x D (Width x Height x Depth) boyutunda bir matristir. Bu boyutlar input chanel a baƒülƒ±dƒ±r. Genellikle sadece W x H boyutlarƒ± kullanƒ±lƒ±r.

Feature map: Convolution i≈ülemi sonrasƒ± elde edilen matristir. Her bir feature map, farklƒ± bir √∂zelliƒüi temsil eder. Eƒüer konvol√ºsyon  mutliple kernel ile yapƒ±lƒ±rsa, her bir kernel i√ßin ayrƒ± bir feature map elde edilir. 

√ñnemli: Feature map, sƒ±radaki konvol√ºsyon/pooling i≈ülemleri i√ßin bir input olur.

Feature map size:  W x H x D boyutunda bir matristir. Feature map'in boyutunu belirler. Bu, kernel boyutuna ve stride'a baƒülƒ±dƒ±r.

Konvol√ºsyon, g√∂r√ºnt√ºn√ºn boyutunu k√º√ß√ºltecektir. Bunun √∂n√ºne ge√ßmek i√ßin, eƒüer stride = 1 ise padding kullanƒ±lƒ±r. 

Stride: Convolution i≈ülemi sƒ±rasƒ±nda kernel'in kaydƒ±rƒ±labilme miktarƒ±dƒ±r. Eƒüer kernel her seferinde 1 piksel kaydƒ±rƒ±lƒ±rsa, stride = 1 olur. 

The stride will affect the effect of feature extraction and the size of the feature map. If the stride is larger, the feature map will be smaller.

Zero padding: Convolution i≈ülemi sƒ±rasƒ±nda g√∂r√ºnt√ºn√ºn boyutunu korumak i√ßin kullanƒ±lƒ±r. G√∂r√ºnt√ºn√ºn √ßevresine sƒ±fƒ±rlar koyulur.

![alt text](image-61.png)
Image invariance: G√∂r√ºnt√ºn√ºn boyutu, konumu veya a√ßƒ±sƒ± deƒüi≈üse bile, modelin aynƒ± √∂zellikleri tanƒ±yabilmesi anlamƒ±na gelir. Bu, konvol√ºsyonel katmanlarƒ±n temel avantajlarƒ±ndan biridir.

![alt text](image-62.png)
Sƒ±radan bir FCN de bu √∂zellik yokken, CNN de vardƒ±r.

![alt text](image-63.png)
Local Receptive Field: Lokalden globale ge√ßer. Her n√∂ron lokal bir b√∂lgeyi i≈üler. Bu, modelin daha az parametre ile daha fazla bilgi yakalamasƒ±nƒ± saƒülar. 

![alt text](image-64.png)
Parameter sharing: Her filtre, hesaplama yaparken aynƒ± aƒüƒ±rlƒ±klarƒ± kullanƒ±r. Bu filtreler g√∂r√ºnt√ºy√º tararken bu aƒüƒ±rlƒ±klar sabittir. Position invariance saƒülar.

# CNN Architecture
![alt text](image-65.png)

Activation function: Convolution i≈ülemi sonrasƒ± elde edilen feature map'in aktivasyon fonksiyonu ile i≈ülenmesi gerekir. Bu, modelin doƒürusal olmayan √∂zellikleri √∂ƒürenmesini saƒülar. Genellikle ReLU (Rectified Linear Unit) kullanƒ±lƒ±r.

## Convolutional Layer
![alt text](image-66.png)
Convolutional layer, g√∂r√ºnt√ºdeki √∂zellikleri √ßƒ±karmak i√ßin kullanƒ±lƒ±r. Bu katman, konvol√ºsyon i≈ülemi ile g√∂r√ºnt√ºy√º i≈üler ve feature map'leri olu≈üturur.

![alt text](image-67.png)
![alt text](image-68.png)
![alt text](image-69.png)
![alt text](image-70.png)
![alt text](image-71.png)

Daha derine gittik√ße, √ßƒ±karƒ±lan √∂zellikler daha tamamlanmƒ±≈ü hale gelir. 5. katmanda, √ßƒ±karƒ±lan √∂zellikler neredeyse resmin tamamƒ±nƒ± kapsar. Farklƒ± g√∂r√ºnt√ºler i√ßin aynƒ± √∂zellikler √ßƒ±karƒ±labilir.

## Pooling Layer
![alt text](image-72.png)
Pooling, Feature map'in boyutunu k√º√ß√ºltmek i√ßin kullanƒ±lƒ±r. Bunu yapmak i√ßin √∂ncelikle bazƒ± b√∂lgelere ayƒ±rƒ±r ve maximum veya average deƒüerini hesaplar. Bu, modelin daha az parametre ile daha fazla bilgi yakalamasƒ±nƒ± saƒülar. Downsampling e benzetebiliriz.

Pooling layer, orijinal g√∂r√ºnt√ºdeki hedef √∂zellikleri elde etmemizi saƒülar. Bu, son sƒ±nƒ±flandƒ±rma i≈ülemi i√ßin √∂nemlidir.

### Max Pooling
![alt text](image-73.png)
Max pooling, feature map'in her b√∂lgesindeki maksimum deƒüeri alƒ±r. Bu, g√∂r√ºnt√ºdeki en belirgin √∂zellikleri yakalamak i√ßin kullanƒ±lƒ±r.

## Fully Connected Layer
![alt text](image-74.png)
Fully connected layer, feature map'in son katmanƒ±dƒ±r. Bu katman, feature map'i d√ºzle≈ütirir (lineer ve uzun bir vekt√∂r) ve her bir n√∂ronun t√ºm n√∂ronlarla baƒülantƒ±lƒ± olduƒüu bir yapƒ±ya d√∂n√º≈üt√ºr√ºr. 

Bu, modelin sƒ±nƒ±flandƒ±rma i≈ülemini ger√ßekle≈ütirmesini saƒülar. Classifier olarak √ßalƒ±≈üƒ±r. Genellikle final sonucu Softmax aktivasyon fonksiyonu ile verir.

# Datasets, Networks, etc.
![alt text](image-75.png)
![alt text](image-76.png)
Burada g√∂r√ºld√ºƒü√º gibi, 2014 yƒ±lƒ±nda Image Classification, insan seviyesine ula≈ütƒ±. Hatta daha altƒ±na bile d√º≈üt√º.

![alt text](image-77.png)
ImagenetNet, 2012 yƒ±lƒ±nda ImageNet Large Scale Visual Recognition Challenge (ILSVRC) i√ßin olu≈üturulmu≈ü bir dataset'tir. Bu dataset, 1000 farklƒ± sƒ±nƒ±fƒ± i√ßerir ve her sƒ±nƒ±f i√ßin binlerce g√∂r√ºnt√º barƒ±ndƒ±rƒ±r.

![alt text](image-78.png)
Alexnet, ReLu'nun sigmoid'den daha iyi performans g√∂sterdiƒüini kanƒ±tladƒ±. 

![alt text](image-79.png)
VGGNet, sadece √ßok k√º√ß√ºk bir kernel kullanarak daha derin bir aƒü olu≈üturdu.

![alt text](image-80.png)
Aradaki en b√ºy√ºk fark, konvol√ºsyon cell'lerin yapƒ±sƒ±dƒ±r.

![alt text](image-81.png)
GoogleNet, 3 adet softmax √ßƒ±kƒ±≈üƒ± i√ßerir. Bu, gradient loss'u √∂nlemek i√ßindir. Ayrƒ±ca Global Average Pooling kullanƒ±r. Inception yapƒ±sƒ± kullanƒ±r.

![alt text](image-82.png)
Inception yapƒ±sƒ±, farklƒ± boyutlarda konvol√ºsyon i≈ülemleri yaparak daha fazla √∂zellik √ßƒ±karmayƒ± saƒülar. 1x1 kernel, aƒüƒ±n non-linearity'sini artƒ±rabilir.

![alt text](image-83.png)
ResNet, residual connection kullanarak daha derin aƒülar olu≈üturmayƒ± saƒülar. Resnetin en b√ºy√ºk amacƒ±, model derinle≈ütik√ße ortaya √ßƒ±kan degenerate problem'i √ß√∂zmektir. 

Degenerate problem, aƒü, √∂rneƒüin 10 katmanda en iyi sonucu verecektir, fakat biz aƒüƒ± 20 katman olarak eƒüittiƒüimizde, 10 katmanlƒ± aƒüdan daha k√∂t√º sonu√ßlar alƒ±rƒ±z. Bu, aƒüƒ±n daha derinle≈ütik√ße √∂ƒürenme yeteneƒüinin azalmasƒ± anlamƒ±na gelir.

![alt text](image-84.png)
Residual connection, bir katmandan sonraki katmana doƒürudan baƒülantƒ± saƒülar. Input ve output'u birle≈ütirir. 

Eƒüer fonksiyon deƒüeri 0 ise, √∂nemli bir √∂zellik √ßƒ±karƒ±lamadƒ±ƒüƒ± anla≈üƒ±lƒ±r. Aƒüƒ±n optimal fit e eri≈ütiƒüini g√∂sterir.

![alt text](image-85.png)
√áalƒ±≈üma, SENet bloƒüu ile ResNeXt bloƒüunu birle≈ütirerek daha iyi bir performans elde etti. SENet, Squeeze-and-Excitation Network olarak bilinir. GAP kullanƒ±r ve Scale operation ile feature map'leri yeniden √∂l√ßeklendirir. 

# Quiz
## True or False
Image calculation includes arithmetic operation and coordinate transformation.(1.0 points) - T
```
#### üî¢ Arithmetic Operations
These include pixel-wise operations such as:

- Addition, subtraction, multiplication, and division (e.g. blending images, brightness adjustment)
- Convolution operations (e.g. applying filters or kernels)
- Normalization or scaling of pixel values

#### üìê Coordinate Transformations
These modify the spatial arrangement of pixels:

- Translation, rotation, scaling, and shearing
- Affine and perspective transformations
- Mapping pixels from one coordinate space to another (e.g. image warping, registration)
```

Traditional image preprocessing methods can be used for data set enhancement.(1.0 points) - T

We can use confusion matrix to measure regression result.(1.0 points) - F
```
A confusion matrix is specifically designed to evaluate the performance of classification models, not regression models. Here's why:

ü§î Why It Doesn't Fit for Regression
- Confusion matrix compares predicted class labels vs true class labels.
- Regression outputs continuous numerical values, not discrete classes.
- There's no straightforward way to define "true positive", "false positive", etc., in the context of continuous values.

‚úÖ What You Use for Regression Instead
You typically use metrics like:

- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- Mean Absolute Error (MAE)
- R¬≤ Score These assess how close the predicted values are to the actual values.
```

Object detection and object segmentation are the same.(1.0 points) - F
```
üß≠ Object Detection
Goal: Identify and locate objects in an image.
Output: Bounding boxes around each detected object with associated class labels.

Example: Drawing rectangles around a dog and a bicycle in a street scene.

üé® Object Segmentation
Goal: Classify every pixel in an image as belonging to an object or background.

Types:
Semantic segmentation: Labels each pixel by class but doesn‚Äôt differentiate between object instances.
Instance segmentation: Labels each pixel and distinguishes between different instances of the same class.

Output: Precise object outlines rather than simple boxes.
```

The sliding window method is commonly used in image precessing.(1.0 points) - T

## Single Choice

When we use gamma correction, the r value is bigger, them image will be ( ).(1.0 points) (A)  
A. Darker   
B. Brighter  
C. No change

```
Gama correction is a nonlinear operation that adjusts the brightness of an image. 
r > 1 -> darker
r < 1 -> brighter
r = 1 -> no change
```

Opening operation means ( ).(1.0 points) (B)  
A. Erosion followed by erosion  
B. Erosion followed by dilation  
C. Dilation followed by dilation  
D. Dilation followed by erosion

```
Opening: Erosion -> Dilation
Closing: Dilation -> Erosion
```

Which of the followings can extract global featuresÔºü(1.0 points) (B)
A. Convolutional kernel in shadow layer
B. Convolutional kernel in deep layer
C. Pooling layer
D. Fully connected layer

```
Shallow layers (early convolutional layers) extract local features, such as edges, corners, and textures, 
because their receptive field (the area of the input image they "see") is small.

Deep layers (later convolutional layers) have a much larger receptive field due to the stacking of multiple layers. 
This means each neuron in a deep layer can "see" a much larger portion of the input image, sometimes even the whole image.

As a result, deep convolutional kernels can extract global features‚Äîsuch as object shapes,
overall structure, or context‚Äîbecause they aggregate information from the entire image.

Summary:

Shallow layers = local features
Deep layers = global features
Pooling and fully connected layers do not directly extract global features;
pooling reduces spatial size,
and fully connected layers use features already extracted by previous layers.
```

Resnet solve ( ) problem.(1.0 points) (A)
A. Degenerate 
B. Gradient explosion 
C. Gradient loss

## Multiple Choices
Which of the followings are computer vision application scenarios?(1.0 points)  (all)
A. Public security    
B. National defense  
C. Smart transportation  
D. Entertainment

Which of the followings are color space?(1.0 points) (A, B)  
A. YUV  
B. HSV  
C. LBP  
D. SVM  

```
‚ùå The incorrect options:
C. LBP (Local Binary Patterns):
Not a color space. It's a texture descriptor used in image processing to classify textures.

D. SVM (Support Vector Machine): 
A machine learning algorithm for classification, not related to color representation.
```

Which of the followings are important convolutional conceptsÔºü(1.0 points) (all)  
A. kernel size  
B. stride  
C. feature map size  
D. zero padding  

